import datetime as datetime
import warnings
from random import choice
from random import randint
from time import sleep
import pandas as pd
import requests
from scrapy import Selector


def main():

    warnings.filterwarnings("ignore")
    pd.set_option('expand_frame_repr', False)
    pd.set_option('display.max_rows', None)
    pd.set_option('display.max_columns', None)
    agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.104 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:84.0) Gecko/20100101 Firefox/84.0',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.2 Safari/605.1.15',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36',
        'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:84.0) Gecko/20100101 Firefox/84.0',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_1_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64; rv:84.0) Gecko/20100101 Firefox/84.0',
        'Mozilla/4.0 (compatible; MSIE 9.0; Windows NT 6.1)',
        'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko',
        'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)',
        'Mozilla/5.0 (Windows NT 6.1; Trident/7.0; rv:11.0) like Gecko',
        'Mozilla/5.0 (Windows NT 6.1; Trident/7.0; rv:11.0) like Gecko',
        'Mozilla/5.0 (Windows NT 6.1; Trident/7.0; rv:11.0) like Gecko',
        'Mozilla/5.0 (Windows NT 6.2; WOW64; Trident/7.0; rv:11.0) like Gecko',
        'Mozilla/5.0 (Windows NT 6.2; WOW64; Trident/7.0; rv:11.0) like Gecko',
        'Mozilla/5.0 (Windows NT 6.2; WOW64; Trident/7.0; rv:11.0) like Gecko',
        'Mozilla/5.0 (Windows NT 6.2; WOW64; Trident/7.0; rv:11.0) like Gecko',
        'Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko',
        'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.0; Trident/5.0)',
        'Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0',
        'Mozilla/5.0 (Windows NT 6.1; Win64; x64; Trident/7.0; rv:11.0) like Gecko',
        'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)',
        'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/6.0)',
        'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
        'AppleWebKit/537.36 (KHTML, like Gecko)',
        'Chrome/63.0.3239.132 Safari/537.36',
        'Mozilla/5.0',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36',
        'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.7 (KHTML, like Gecko) Chrome/16.0.912.36 Safari/535.7',
        'Mozilla/5.0 (Windows NT 6.2; Win64; x64; rv:16.0) Gecko/16.0 Firefox/16.0',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/534.55.3 (KHTML, like Gecko) Version/5.1.3 Safari/534.53.10']


    df_offers_apple_clean = pd.read_csv('csv_data/profiles_offers_apple_clean.csv')
    df_offers_apple_clean = df_offers_apple_clean[(len(pd.read_csv('csv_data/checked_results.csv'))-1):]    # hierfür ist das x nützlich, da muss ich bei abbruch nicht von vorne anfangen
    x = len(pd.read_csv('csv_data/checked_results.csv'))-1
    for i in range(0, 1):
        for i in df_offers_apple_clean['offer-ID']:
            #i = 2294764916
            j = ("http://api.scraperapi.com?api_key=cf1dfe285ad7b62e5387b4ad6e3fe1b5&url=https://www.ebay-kleinanzeigen.de/s-anzeige/{}".format(i))
            print(j.replace('http://api.scraperapi.com?api_key=cf1dfe285ad7b62e5387b4ad6e3fe1b5&url=', ''))
            user_agent = choice(agents)
            headers = {'User-Agent': user_agent}
            html = requests.get(url=j, headers=headers).content
            sel = Selector(text=html)
            selled_or_self_deleted = sel.css('#viewad-action-prnt > span:nth-child(2)::text').extract()  # wenn sich noch nachrichten schreiben lassen

            if len(selled_or_self_deleted) > 0:
                result = 2                          # existiert immer noch
            else:
                title = sel.css('#viewad-description::text').extract()    # wenn sich keine nachrichten schreiben lassen, aber anzeige noch existiert
                person = sel.css('.text-body-regular-strong > a:nth-child(1)::text').extract()
                if len(title) > 0 or len(person) > 0:
                    result = 0                      # vom user gelöscht oder verkauft
                else:
                    result = 1                      # von kleinanzeigen gelöscht; --> scam / Verstoß gegen Nutzungsbedingungen --> weiterleitung auf startseite

            df = pd.DataFrame()
            df['fraud'] = [result]
            df['offer-ID'] = [i]
            df['scraptime'] = datetime.datetime.now()

            print(x+3)                                # indexnummer (beschreibt Fortschritt)
            x = x+1

            df1 = pd.read_csv('csv_data/checked_results.csv')
            df1 = df1.append(df)
            df1 = df1.drop_duplicates(subset=['offer-ID'], keep='last')
            df1.to_csv('csv_data/checked_results.csv', index=False)

            sleeptime = float(randint(0, 0))
            sleep(sleeptime)

    # '''


main()